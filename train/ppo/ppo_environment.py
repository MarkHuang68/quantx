# æª”æ¡ˆ: train/ppo/ppo_environment.py

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import StandardScaler

import settings
from utils.common import create_features_trend

class TradingEnvironment(gym.Env):
    def __init__(self, df_data, initial_balance=10000, leverage=5, commission=0.0004, reward_window_size=100):
        super().__init__()

        self.df_data = df_data
        self.initial_balance = initial_balance
        self.leverage = leverage
        self.commission = commission
        self.reward_window_size = reward_window_size # æ–°å¢žï¼šè¨ˆç®—æ³¢å‹•æ€§çš„çª—å£å¤§å°

        # ç’°å¢ƒåƒæ•¸
        self.features = [col for col in df_data.columns if col not in ['Open', 'High', 'Low', 'Close']]
        self.n_features = len(self.features)
        self.max_timesteps = len(self.df_data) - 1

        # å‹•ä½œç©ºé–“
        self.action_map = np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0], dtype=np.float32)
        self.action_space = spaces.Discrete(len(self.action_map))

        # è§€å¯Ÿç‹€æ…‹ç©ºé–“
        obs_low = [-np.inf] * (self.n_features - 1) + [-1, -1, 0]
        obs_high = [np.inf] * (self.n_features - 1) + [1, 1, np.inf]
        self.observation_space = spaces.Box(low=np.array(obs_low), high=np.array(obs_high), dtype=np.float32)

        # å…§éƒ¨ç‹€æ…‹
        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 0
        self.balance = self.initial_balance
        self.net_worth = self.initial_balance
        self.max_net_worth = self.initial_balance # æ–°å¢žï¼šç”¨æ–¼è¨ˆç®—æœ€å¤§å›žæ’¤
        self.position = 0.0
        self.entry_price = 0.0
        self.pnl_history = [] # æ–°å¢žï¼šè¨˜éŒ„ PnL æ­·å²
        self.net_worth_history = [] # æ–°å¢žï¼šè¨˜éŒ„æ·¨å€¼æ­·å²
        return self._get_observation(), {}

    def step(self, action):
        target_position = self.action_map[action]
        price_now = self.df_data['Close'].iloc[self.current_step]
        price_next = self.df_data['Close'].iloc[self.current_step + 1]

        # è¨ˆç®—åŽŸå§‹ PnL
        price_change_ratio = (price_next - price_now) / price_now
        pnl_pct = self.position * price_change_ratio * self.leverage
        self.pnl_history.append(pnl_pct)

        # æ›´æ–°ç‹€æ…‹
        self.net_worth *= (1 + pnl_pct)
        self.net_worth_history.append(self.net_worth)
        self.max_net_worth = max(self.max_net_worth, self.net_worth) # æ›´æ–°æœ€é«˜æ·¨å€¼

        # --- å…¨æ–°çš„é¢¨éšªèª¿æ•´å¾ŒçŽå‹µæ©Ÿåˆ¶ ---
        reward = self._calculate_risk_adjusted_reward(pnl_pct, target_position)

        # æ›´æ–°å€‰ä½
        self.position = target_position
        self.entry_price = price_now if target_position != 0 else 0

        self.current_step += 1
        done = self.current_step >= self.max_timesteps or self.net_worth < self.initial_balance * 0.5

        # åœ¨ episode çµæŸæ™‚ï¼Œå¢žåŠ å°æœ€å¤§å›žæ’¤çš„æ‡²ç½°
        if done:
            max_drawdown = (self.max_net_worth - self.net_worth) / self.max_net_worth
            # æ‡²ç½°èˆ‡å›žæ’¤çš„å¹³æ–¹æˆæ­£æ¯”ï¼Œä»¥åŠ é‡å°å¤§å¹…åº¦å›žæ’¤çš„æ‡²ç½°
            drawdown_penalty = 5 * (max_drawdown ** 2)
            reward -= drawdown_penalty

        return self._get_observation(), reward, done, False, {'net_worth': self.net_worth}

    def _calculate_risk_adjusted_reward(self, pnl_pct, target_position):
        """
        è¨ˆç®—è€ƒæ…®äº†é¢¨éšªçš„çŽå‹µã€‚
        """
        # äº¤æ˜“æˆæœ¬æ‡²ç½°
        trade_cost = abs(target_position - self.position) * self.commission

        # ä½¿ç”¨å¤æ™®æ¯”çŽ‡çš„æ€æƒ³èª¿æ•´çŽå‹µ
        if len(self.pnl_history) < self.reward_window_size:
            # åœ¨æ­·å²æ•¸æ“šä¸è¶³æ™‚ï¼Œä½¿ç”¨ç°¡åŒ–çŽå‹µ
            return pnl_pct - trade_cost

        # è¨ˆç®—è¿‘æœŸ PnL çš„æ³¢å‹•æ€§ (æ¨™æº–å·®)
        recent_pnl = self.pnl_history[-self.reward_window_size:]
        pnl_std = np.std(recent_pnl) + 1e-6 # åŠ ä¸Šä¸€å€‹æ¥µå°å€¼é¿å…é™¤ä»¥é›¶

        # å¤æ™®çŽå‹µï¼šå›žå ±èˆ‡é¢¨éšªçš„æ¯”å€¼
        sharpe_reward = pnl_pct / pnl_std

        return sharpe_reward - trade_cost

    def _get_observation(self):
        features = self.df_data[self.features].iloc[self.current_step].values
        account_state = np.array([self.position, self.net_worth / self.initial_balance])
        return np.concatenate([features, account_state]).astype(np.float32)

def prepare_data_for_ppo(symbol, ohlcv_data):
    """
    ç‚º PPO è¨“ç·´æº–å‚™æ•¸æ“šï¼ŒåŒ…æ‹¬è¨ˆç®—ä¸¦æ¨™æº–åŒ– XGBoost è¨Šè™Ÿã€‚
    """
    print(f"--- æ­£åœ¨ç‚º {symbol} æº–å‚™ PPO è¨“ç·´æ•¸æ“š ---")

    try:
        model_path = settings.get_trend_model_path(symbol, '1m', settings.TREND_MODEL_VERSION)
        model = xgb.XGBClassifier()
        model.load_model(model_path)
    except Exception as e:
        print(f"ðŸ›‘ éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥ {symbol} çš„ XGBoost æ¨¡åž‹ã€‚è«‹å…ˆè¨“ç·´æ¨¡åž‹ã€‚ {e}")
        return None

    df_features, features_list = create_features_trend(ohlcv_data.copy())
    raw_signal = model.predict(df_features[features_list]).astype(int)

    signal_map = {1: 1, 2: -1, 0: 0}
    df_features['xgb_signal'] = pd.Series(raw_signal, index=df_features.index).map(signal_map)

    ppo_features = features_list + ['xgb_signal', 'Close']
    df_ppo = df_features[ppo_features].dropna()

    scaler = StandardScaler()
    df_ppo[features_list] = scaler.fit_transform(df_ppo[features_list])

    return df_ppo
