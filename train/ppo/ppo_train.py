# æª”æ¡ˆ: train/ppo/ppo_train.py

import os
import time
import argparse
import sys
import pandas as pd
from multiprocessing import cpu_count

# ç¢ºä¿å¯ä»¥å¼•ç”¨åˆ°ä¸Šå±¤ç›®éŒ„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from core.data_loader import load_csv_data
from train.ppo.ppo_environment import TradingEnvironment, prepare_data_for_ppo
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.utils import set_random_seed

from settings import SYMBOLS_TO_TRADE

DATA_DIR = "data"

PPO_HYPERPARAMS = {
    "n_steps": 2048,
    "batch_size": 64,
    "gamma": 0.99,
    "learning_rate": 0.0003,
    "verbose": 1
}

def make_env(df, rank, seed=0):
    """
    SubprocVecEnv çš„ç’°å¢ƒç”¢ç”Ÿå™¨ã€‚
    """
    def _init():
        env = TradingEnvironment(df)
        env.reset(seed=seed + rank)
        return env
    set_random_seed(seed)
    return _init

def train_ppo_agent(total_timesteps=1_000_000, output_dir="ppo_models"):
    """
    è¼‰å…¥æ‰€æœ‰äº¤æ˜“å°çš„æ•¸æ“šï¼Œæº–å‚™çµ±ä¸€çš„ç’°å¢ƒï¼Œä¸¦è¨“ç·´ä¸€å€‹å…±ç”¨çš„ PPO æ™ºèƒ½é«”ã€‚
    """
    run_name = f"ppo_agent_UNIFIED_{time.strftime('%Y%m%d_%H%M%S')}"
    log_dir = os.path.join(output_dir, "logs", run_name)

    print(f"\n=======================================================")
    print(f"--- é–‹å§‹è¨“ç·´çµ±ä¸€çš„ PPO é¢¨éšªç®¡ç†ä»£ç† ---")
    print(f"--- äº¤æ˜“å°: {SYMBOLS_TO_TRADE} ---")
    print(f"=======================================================")

    # 1. è¼‰å…¥ä¸¦æº–å‚™æ‰€æœ‰äº¤æ˜“å°çš„æ•¸æ“š
    all_symbols_data = []
    for symbol in SYMBOLS_TO_TRADE:
        # å‡è¨­æ•¸æ“šæª”æ¡ˆå‘½åæ ¼å¼ç‚º: {DATA_DIR}/{SYMBOL_pair}_{timeframe}.csv, e.g., data/ETHUSDT_1m.csv
        # é€™è£¡æˆ‘å€‘éœ€è¦ä¸€å€‹çµ±ä¸€çš„æ•¸æ“šæª”æ¡ˆå‘½åç´„å®š
        csv_path = os.path.join(DATA_DIR, f"{symbol.replace('/', '')}_1m.csv")

        print(f"\n--- æ­£åœ¨è™•ç† {symbol} çš„æ•¸æ“š ---")
        if not os.path.exists(csv_path):
            print(f"ğŸ›‘ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {symbol} çš„æ•¸æ“šæª”æ¡ˆï¼š{csv_path}ï¼Œå·²è·³éã€‚")
            continue

        raw_data = load_csv_data(csv_path, symbol=symbol)
        if raw_data is None:
            continue

        df_ppo = prepare_data_for_ppo(symbol, raw_data)
        if df_ppo is not None:
            all_symbols_data.append(df_ppo)

    if not all_symbols_data:
        print("ğŸ›‘ éŒ¯èª¤ï¼šæ²’æœ‰ä»»ä½•æ•¸æ“šå¯ä¾›è¨“ç·´ã€‚è«‹æª¢æŸ¥æ•¸æ“šæª”æ¡ˆã€‚")
        return

    # 2. åˆä½µæ‰€æœ‰æ•¸æ“šé›†
    print("\n--- æ­£åœ¨åˆä½µæ‰€æœ‰äº¤æ˜“å°çš„æ•¸æ“šé›† ---")
    unified_df = pd.concat(all_symbols_data, ignore_index=True)
    print(f"âœ… çµ±ä¸€æ•¸æ“šé›†å‰µå»ºå®Œç•¢ï¼Œç¸½å…± {len(unified_df)} ç­†æ•¸æ“šã€‚")

    # 3. å‰µå»ºå¤šæ ¸å¿ƒ PPO ç’°å¢ƒ
    try:
        num_cpu = cpu_count()
        print(f"--- åµæ¸¬åˆ° {num_cpu} å€‹ CPU æ ¸å¿ƒï¼Œå°‡ç”¨æ–¼å¹³è¡ŒåŒ–è¨“ç·´ ---")
        env = SubprocVecEnv([make_env(unified_df, i) for i in range(num_cpu)])
    except Exception as e:
        print(f"ğŸ›‘ éŒ¯èª¤ï¼šç„¡æ³•å‰µå»º SubprocVecEnv ç’°å¢ƒã€‚{e}")
        return

    # 4. è¨­å®šå›èª¿
    checkpoint_callback = CheckpointCallback(
        save_freq=max(100000 // num_cpu, 1),
        save_path=os.path.join(output_dir, "checkpoints"),
        name_prefix="ppo_checkpoint_UNIFIED"
    )

    # 5. å»ºç«‹ä¸¦è¨“ç·´ PPO æ¨¡å‹
    model = PPO("MlpPolicy", env, **PPO_HYPERPARAMS, seed=42, tensorboard_log=log_dir)

    print(f"--- PPO æ™ºèƒ½é«”é–‹å§‹å­¸ç¿’ ({total_timesteps} æ­¥) ---")
    start_time = time.time()
    model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback)
    training_time = time.time() - start_time
    print(f"\n--- è¨“ç·´å®Œæˆï¼ç¸½è€—æ™‚: {training_time:.2f} ç§’ ---")

    # 6. å„²å­˜æœ€çµ‚æ¨¡å‹
    final_save_path = os.path.join(output_dir, "ppo_agent_UNIFIED_final.zip")
    os.makedirs(output_dir, exist_ok=True)
    model.save(final_save_path)
    print(f"âœ… çµ±ä¸€ PPO æ™ºèƒ½é«”å„²å­˜å®Œç•¢ï¼š{final_save_path}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='çµ±ä¸€ PPO æ™ºèƒ½é«”å¹³è¡ŒåŒ–è¨“ç·´è…³æœ¬')
    parser.add_argument('-t', '--timesteps', type=int, default=2_000_000, help='ç¸½è¨“ç·´æ­¥æ•¸ (é è¨­: 2,000,000)')
    args = parser.parse_args()

    train_ppo_agent(args.timesteps)
