# æª”æ¡ˆ: train/ppo/ppo_train.py

import os
import time
import argparse
import sys
import numpy as np

# ç¢ºä¿å¯ä»¥å¼•ç”¨åˆ°ä¸Šå±¤ç›®éŒ„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from core.data_loader import load_csv_data
from train.ppo.ppo_environment import TradingEnvironment, prepare_data_for_ppo
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback
from settings import SYMBOLS_TO_TRADE

PPO_HYPERPARAMS = {
    "n_steps": 2048,
    "batch_size": 64,
    "gamma": 0.99,
    "learning_rate": 0.0003,
    "verbose": 1
}

def train_ppo_agent(symbol, csv_path, total_timesteps=1_000_000, output_dir="ppo_models"):
    """
    è¼‰å…¥æ•¸æ“šã€æº–å‚™ç’°å¢ƒä¸¦è¨“ç·´ PPO æ™ºèƒ½é«”ã€‚
    """
    symbol_str = symbol.replace('/', '_')
    run_name = f"ppo_agent_{symbol_str}_{time.strftime('%Y%m%d_%H%M%S')}"
    log_dir = os.path.join(output_dir, "logs", run_name)

    print(f"\n=======================================================")
    print(f"--- é–‹å§‹è¨“ç·´ PPO ä»£ç†: {symbol} ---")
    print(f"=======================================================")

    # 1. è¼‰å…¥ä¸¦æº–å‚™æ•¸æ“š
    raw_data = load_csv_data(csv_path, symbol=symbol)
    if raw_data is None:
        return

    df_ppo = prepare_data_for_ppo(symbol, raw_data)
    if df_ppo is None:
        return

    # 2. å‰µå»º PPO ç’°å¢ƒ
    try:
        env = DummyVecEnv([lambda: TradingEnvironment(df_ppo)])
    except Exception as e:
        print(f"ğŸ›‘ éŒ¯èª¤ï¼šç„¡æ³•å‰µå»º TradingEnvironmentã€‚{e}")
        return

    # 3. è¨­å®šå›èª¿
    checkpoint_callback = CheckpointCallback(
        save_freq=100000,
        save_path=os.path.join(output_dir, "checkpoints"),
        name_prefix=f"ppo_checkpoint_{symbol_str}"
    )

    # 4. å»ºç«‹ä¸¦è¨“ç·´ PPO æ¨¡å‹
    model = PPO("MlpPolicy", env, **PPO_HYPERPARAMS, seed=42, tensorboard_log=log_dir)

    print(f"--- PPO æ™ºèƒ½é«”é–‹å§‹å­¸ç¿’ ({total_timesteps} æ­¥) ---")
    start_time = time.time()
    model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback)
    training_time = time.time() - start_time
    print(f"\n--- è¨“ç·´å®Œæˆï¼ç¸½è€—æ™‚: {training_time:.2f} ç§’ ---")

    # 5. å„²å­˜æœ€çµ‚æ¨¡å‹
    final_save_path = os.path.join(output_dir, f"ppo_agent_{symbol_str}_final.zip")
    os.makedirs(output_dir, exist_ok=True)
    model.save(final_save_path)
    print(f"âœ… PPO æ™ºèƒ½é«”å„²å­˜å®Œç•¢ï¼š{final_save_path}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='PPO æ™ºèƒ½é«”è¨“ç·´è…³æœ¬')
    parser.add_argument('-s', '--symbol', type=str, required=True, help='è¦è¨“ç·´çš„äº¤æ˜“å° (ä¾‹å¦‚: ETH/USDT)')
    parser.add_argument('--csv', type=str, required=True, help='åŒ…å«æ­·å² K ç·šæ•¸æ“šçš„ CSV æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('-t', '--timesteps', type=int, default=1_000_000, help='ç¸½è¨“ç·´æ­¥æ•¸ (é è¨­: 1,000,000)')

    args = parser.parse_args()

    if args.symbol in SYMBOLS_TO_TRADE:
        train_ppo_agent(args.symbol, args.csv, args.timesteps)
    else:
        print(f"ğŸ›‘ éŒ¯èª¤ï¼šè«‹ä½¿ç”¨ config.py ä¸­å®šç¾©çš„äº¤æ˜“å°ã€‚")
        print(f"å¯ç”¨äº¤æ˜“å°: {SYMBOLS_TO_TRADE}")
