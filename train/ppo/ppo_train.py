# æª”æ¡ˆ: train/ppo/ppo_train.py

import os
import time
import argparse
import sys
from multiprocessing import cpu_count

# ç¢ºä¿å¯ä»¥å¼•ç”¨åˆ°ä¸Šå±¤ç›®éŒ„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from utils.common import fetch_data
from train.ppo.ppo_environment import TradingEnvironment, prepare_data_for_ppo
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback
from settings import SYMBOLS_TO_TRADE

PPO_HYPERPARAMS = {
    "n_steps": 2048,
    "batch_size": 64,
    "gamma": 0.99,
    "learning_rate": 0.0003,
    "verbose": 1
}

def make_env(df_ppo):
    """
    è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼ SubprocVecEnv åºåˆ—åŒ–ç’°å¢ƒå»ºç«‹éç¨‹ã€‚
    """
    def _init():
        return TradingEnvironment(df_ppo)
    return _init

def train_unified_ppo_agent(timeframe, start_date, end_date, total_timesteps=2_000_000, output_dir="ppo_models"):
    """
    ç‚º SYMBOLS_TO_TRADE ä¸­çš„æ‰€æœ‰äº¤æ˜“å°è¼‰å…¥æ•¸æ“šï¼Œä¸¦è¡Œè¨“ç·´ä¸€å€‹çµ±ä¸€çš„ PPO æ¨¡å‹ã€‚
    """
    run_name = f"ppo_agent_unified_{timeframe}_{time.strftime('%Y%m%d_%H%M%S')}"
    log_dir = os.path.join(output_dir, "logs", run_name)
    os.makedirs(log_dir, exist_ok=True)

    print(f"\n=======================================================")
    print(f"--- é–‹å§‹ç‚ºæ‰€æœ‰äº¤æ˜“å°è¨“ç·´çµ±ä¸€ PPO æ¨¡å‹ ({timeframe}) ---")
    print(f"--- æ™‚é–“ç¯„åœ: {start_date} to {end_date} ---")
    print(f"--- äº¤æ˜“å°: {SYMBOLS_TO_TRADE} ---")
    print(f"=======================================================")

    # 1. ç‚ºæ¯å€‹äº¤æ˜“å°è¼‰å…¥æ•¸æ“šä¸¦å»ºç«‹ç’°å¢ƒ
    env_makers = []
    for symbol in SYMBOLS_TO_TRADE:
        print(f"\n--- æ­£åœ¨è™•ç†äº¤æ˜“å°: {symbol} ---")

        # ä½¿ç”¨æ–°çš„æ™‚é–“åƒæ•¸ä¾†ç²å–æ•¸æ“š
        raw_data = fetch_data(symbol=symbol, start_date=start_date, end_date=end_date, timeframe=timeframe)
        if raw_data is None or raw_data.empty:
            print(f"ğŸ›‘ è­¦å‘Š: {symbol} åœ¨æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„æ•¸æ“šç„¡æ³•è¼‰å…¥ï¼Œå°‡è·³éã€‚")
            continue

        df_ppo = prepare_data_for_ppo(symbol, raw_data)
        if df_ppo is None:
            print(f"ğŸ›‘ è­¦å‘Š: {symbol} çš„ PPO æ•¸æ“šæº–å‚™å¤±æ•—ï¼Œå°‡è·³éã€‚")
            continue

        env_makers.append(make_env(df_ppo))

    if not env_makers:
        print("ğŸ›‘ éŒ¯èª¤: æ²’æœ‰å¯ç”¨æ–¼è¨“ç·´çš„ç’°å¢ƒã€‚è«‹æª¢æŸ¥æ•¸æ“šå’Œ XGBoost æ¨¡å‹ã€‚")
        return

    # 2. å»ºç«‹ä¸¦è¡ŒåŒ–çš„ PPO å‘é‡ç’°å¢ƒ
    num_cpu = min(cpu_count(), len(env_makers))
    print(f"\n--- ä½¿ç”¨ {num_cpu} å€‹ CPU æ ¸å¿ƒé€²è¡Œä¸¦è¡Œè¨“ç·´ ---")
    try:
        env = SubprocVecEnv(env_makers)
    except Exception as e:
        print(f"ğŸ›‘ éŒ¯èª¤ï¼šç„¡æ³•å‰µå»º SubprocVecEnvã€‚{e}")
        return

    # 3. è¨­å®šå›èª¿
    checkpoint_callback = CheckpointCallback(
        save_freq=100000,
        save_path=os.path.join(output_dir, "checkpoints"),
        name_prefix=f"ppo_checkpoint_unified_{timeframe}"
    )

    # 4. å»ºç«‹ä¸¦è¨“ç·´çµ±ä¸€çš„ PPO æ¨¡å‹
    model = PPO("MlpPolicy", env, **PPO_HYPERPARAMS, seed=42, tensorboard_log=log_dir)

    print(f"--- çµ±ä¸€ PPO æ¨¡å‹é–‹å§‹å­¸ç¿’ ({total_timesteps} æ­¥) ---")
    start_time = time.time()
    model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback)
    training_time = time.time() - start_time
    print(f"\n--- è¨“ç·´å®Œæˆï¼ç¸½è€—æ™‚: {training_time:.2f} ç§’ ---")

    # 5. å„²å­˜æœ€çµ‚çš„çµ±ä¸€æ¨¡å‹
    final_save_path = os.path.join(output_dir, f"ppo_agent_unified_{timeframe}_final.zip")
    model.save(final_save_path)
    print(f"âœ… çµ±ä¸€ PPO æ¨¡å‹å„²å­˜å®Œç•¢ï¼š{final_save_path}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='çµ±ä¸€ PPO æ¨¡å‹è¨“ç·´è…³æœ¬')
    parser.add_argument('-tf', '--timeframe', type=str, required=True, help='è¦è¨“ç·´çš„æ™‚é–“é€±æœŸ (ä¾‹å¦‚: 5m, 1h)')
    parser.add_argument('-sd', '--start', type=str, required=True, help='è¨“ç·´èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('-ed', '--end', type=str, required=True, help='è¨“ç·´çµæŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('-t', '--timesteps', type=int, default=2_000_000, help='ç¸½è¨“ç·´æ­¥æ•¸ (é è¨­: 2,000,000)')

    args = parser.parse_args()

    train_unified_ppo_agent(args.timeframe, args.start, args.end, args.timesteps)
