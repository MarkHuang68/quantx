# æª”æ¡ˆ: ppo_factory/ppo_train.py

import os
import time
import argparse
import sys
import numpy as np

# ç¢ºä¿å¯ä»¥å¼•ç”¨åˆ°ä¸Šå±¤ç›®éŒ„çš„ config
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# --- 1. å¼•ç”¨æ ¸å¿ƒå·¥å…· ---
import config
from ppo_environment import TradingEnvironment # <-- å¼•ç”¨æˆ‘å€‘å‰›å‰›å‰µå»ºçš„ç’°å¢ƒ

# --- 2. å¼•ç”¨ Stable-Baselines3 (PPO) ---
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback

# --- 3. è¨“ç·´åƒæ•¸è¨­å®š ---
PPO_HYPERPARAMS = {
    "n_steps": 2048,          # æ”¶é›†æ•¸æ“šçš„æ­¥æ•¸
    "batch_size": 64,         # å„ªåŒ–ä½¿ç”¨çš„æ•¸æ“šé‡
    "gamma": 0.99,            # é•·æœŸçå‹µæŠ˜æ‰£
    "learning_rate": 0.0003,
    "verbose": 1
}

def train_ppo_agent(symbol, total_timesteps=1_000_000, output_dir="ppo_models"):
    """
    è¼‰å…¥äº¤æ˜“ç’°å¢ƒä¸¦è¨“ç·´ PPO æ™ºèƒ½é«”ã€‚
    """
    symbol_str = symbol.replace('/', '_')
    run_name = f"ppo_agent_{symbol_str}_{time.strftime('%Y%m%d_%H%M%S')}"
    log_dir = os.path.join(output_dir, "logs", run_name)
    
    print(f"\n=======================================================")
    print(f"--- éšæ®µ 2: å•Ÿå‹• PPO æ±ºç­–å·¥å»  - è¨“ç·´ {symbol} ---")
    print(f"=======================================================")

    # 1. å‰µå»ºç’°å¢ƒ
    try:
        # PPO è¦æ±‚ç’°å¢ƒå¿…é ˆæ˜¯å‘é‡åŒ–çš„ï¼Œå³ä½¿åªæœ‰ä¸€å€‹ç’°å¢ƒ
        env = DummyVecEnv([lambda: TradingEnvironment(
            symbol=symbol,
            initial_balance=10000,
            leverage=5, 
            commission=0.0004
        )])
    except Exception as e:
        print(f"ğŸ›‘ éŒ¯èª¤ï¼šç„¡æ³•å‰µå»º TradingEnvironmentã€‚è«‹æª¢æŸ¥å°ˆå®¶æ¨¡å‹å’Œæ•¸æ“šã€‚{e}")
        return

    # 2. å»ºç«‹å›èª¿ (Callback) ä»¥ä¾¿ä¿å­˜ä¸­é–“æª¢æŸ¥é»
    checkpoint_callback = CheckpointCallback(
        save_freq=100000,  # æ¯ 10 è¬æ­¥ä¿å­˜ä¸€æ¬¡
        save_path=os.path.join(output_dir, "checkpoints"),
        name_prefix=f"ppo_checkpoint_{symbol_str}"
    )

    # 3. å»ºç«‹ PPO æ¨¡å‹
    model = PPO("MlpPolicy", env, **PPO_HYPERPARAMS, seed=42, tensorboard_log=log_dir)

    # 4. è¨“ç·´æ™ºèƒ½é«” (é€™å°‡åœ¨æ­·å²æ•¸æ“šä¸Šæ¨¡æ“¬äº¤æ˜“ 100 è¬æ¬¡)
    print(f"--- PPO æ™ºèƒ½é«”é–‹å§‹å­¸ç¿’ ({total_timesteps} æ­¥) ---")
    
    start_time = time.time()
    model.learn(
        total_timesteps=total_timesteps,
        callback=checkpoint_callback,
        reset_num_timesteps=False
    )
    training_time = time.time() - start_time
    print(f"\n--- è¨“ç·´å®Œæˆï¼ç¸½è€—æ™‚: {training_time:.2f} ç§’ ---")

    # 5. å„²å­˜æœ€çµ‚æ¨¡å‹
    final_save_path = os.path.join(output_dir, f"ppo_agent_{symbol_str}_final.zip")
    os.makedirs(output_dir, exist_ok=True)
    model.save(final_save_path)
    print(f"âœ… PPO æ™ºèƒ½é«”å„²å­˜å®Œç•¢ï¼š{final_save_path}")

    # 6. æœ€çµ‚æ€§èƒ½æ¸¬è©¦ (åœ¨è¨“ç·´æ•¸æ“šä¸Šè·‘ä¸€æ¬¡ï¼ŒæŸ¥çœ‹æœ€çµ‚æ·¨å€¼)
    obs, _ = env.reset()
    final_net_worth = 0
    for i in range(env.envs[0].max_timesteps):
        action, _states = model.predict(obs, deterministic=True) 
        obs, reward, done, info = env.step(action)
        if done[0]:
            final_net_worth = info[0]['net_worth']
            break
            
    print(f"\n--- PPO æœ€çµ‚æ€§èƒ½æ¸¬è©¦ ---")
    print(f"åˆå§‹è³‡é‡‘: {env.envs[0].initial_balance}")
    print(f"æœ€çµ‚æ·¨å€¼: {final_net_worth:.2f}")
    print(f"ç¸½å ±é…¬ç‡: {((final_net_worth / env.envs[0].initial_balance) - 1) * 100:.2f}%")


if __name__ == '__main__':
    # --- å‘½ä»¤è¡Œåƒæ•¸ ---
    parser = argparse.ArgumentParser(description='PPO æ™ºèƒ½é«”è¨“ç·´å·¥å» ')
    parser.add_argument('-s', '--symbol', type=str, required=True, help='è¦è¨“ç·´çš„äº¤æ˜“å° (ä¾‹å¦‚: ETH/USDT)')
    parser.add_argument('-t', '--timesteps', type=int, default=1_000_000, help='ç¸½æ¨¡æ“¬äº¤æ˜“æ­¥æ•¸ (é è¨­: 1,000,000)')
    
    args = parser.parse_args()
    
    # --- åŸ·è¡Œå¤šè³‡ç”¢è¨“ç·´ ---
    if args.symbol.upper() == 'ALL':
        for symbol in config.SYMBOLS_TO_TRADE:
            train_ppo_agent(symbol, args.timesteps)
    elif args.symbol in config.SYMBOLS_TO_TRADE:
        train_ppo_agent(args.symbol, args.timesteps)
    else:
        print(f"ğŸ›‘ éŒ¯èª¤ï¼šè«‹ä½¿ç”¨ 'ALL' æˆ– config.py ä¸­å®šç¾©çš„äº¤æ˜“å°ã€‚")
        print(f"å¯ç”¨äº¤æ˜“å°: {config.SYMBOLS_TO_TRADE}")