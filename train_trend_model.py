# æª”æ¡ˆ: train_trend_model.py
import json
import warnings
import pandas as pd
import numpy as np
import argparse
import matplotlib.pyplot as plt
import os

# --- 1. å¼•ç”¨ã€Œè¨­å®šæª”ã€å’Œã€Œå…±ç”¨å·¥å…·ç®±ã€ ---
import config
from common_utils import fetch_data, create_features_trend, create_sequences
from hyperparameter_search import SearchIterator

# --- (åŒ¯å…¥æ‰€æœ‰ Keras/Sklearn å·¥å…·) ---
import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Bidirectional
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay

# (è¨­ç½® Keras/Tensorflow çš„éš¨æ©Ÿç¨®å­)
tf.random.set_seed(42)
np.random.seed(42)
warnings.simplefilter(action='ignore', category=FutureWarning)
#ema=20, sma=60, rsi=14, bbands=10
# --- æ‚¨çš„å°‹åƒç©ºé–“ (ä¿æŒä¸è®Š) ---
TREND_SEARCH_SPACE = {
    'ema': [5, 20, 5],
    'sma': [20, 100, 20],
    'rsi': [7, 14, 7],
    'bbands': [2, 8, 6]
}

# --- æ‚¨çš„ LSTM è¨“ç·´åƒæ•¸ (ä¿æŒä¸è®Š) ---
LSTM_BASE_PARAMS = {
    'epochs': 50,
    'batch_size': 64,
    'shuffle': False,
    'callbacks': [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
}

def build_and_train_lstm(df_features, features_list):
    """
    (é€™æ˜¯æˆ‘å€‘ 90.9% çš„å† è»æ¨¡å‹è¨“ç·´é‚è¼¯)
    """
    if df_features is None: return None, 0.0, None, None

    # --- 2. å¾ã€Œè¨­å®šæª”ã€è®€å–æ‰€æœ‰åƒæ•¸ ---
    P = config.TREND_MODEL_PARAMS
    lookback_window = P['LOOKBACK_WINDOW']
    forecast_horizon = P['FORECAST_HORIZON']
    u1, u2, d1 = P['LSTM_UNITS_1'], P['LSTM_UNITS_2'], P['DENSE_UNITS']

    # 1. å®šç¾©ç‰¹å¾µå’Œç›®æ¨™ (å¿…é ˆ 100% åŒ¹é… common_utils.py)

    df_model = df_features.copy()
    
    print(f"\n--- æ­£åœ¨å»ºç«‹ç›®æ¨™: é æ¸¬ {forecast_horizon} å°æ™‚ä¹‹å¾Œçš„è¶¨å‹¢èµ°å‘ ---")
    df_model['target'] = (df_model['SMA'].shift(-forecast_horizon) > df_model['SMA']).astype(int)
    df_model = df_model.dropna() 
    
    # 2. æ¨™æº–åŒ–
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_features = scaler.fit_transform(df_model[features_list])
    target = df_model['target'].values
    
    # 3. å»ºç«‹ 3D åºåˆ—
    X_seq, y_seq = create_sequences(scaled_features, target, lookback_window=lookback_window)
    
    # 4. åˆ†å‰²è³‡æ–™
    test_size = 0.2 
    split_index = int(len(X_seq) * (1 - test_size))
    X_train, X_test = X_seq[:split_index], X_seq[split_index:]
    y_train, y_test = y_seq[:split_index], y_seq[split_index:]

    print(f"è¨“ç·´é›†ç­†æ•¸: {len(X_train)}, æ¸¬è©¦é›†ç­†æ•¸: {len(X_test)}")

    # 5. å»ºç«‹ã€Œæ·±åº¦å †ç–Šã€LSTM æ¨¡å‹
    print("\n--- æ­¥é©Ÿ 4: æ­£åœ¨å»ºç«‹ã€Œæ·±åº¦å †ç–Šã€LSTM æ¨¡å‹... ---")
    model = Sequential()
    model.add(Bidirectional(LSTM(units=u1, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))))
    model.add(Dropout(0.3)) 
    model.add(Bidirectional(LSTM(units=u2)))
    model.add(Dropout(0.3))
    model.add(Dense(units=d1, activation='relu')) 
    model.add(Dense(units=1, activation='sigmoid'))

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.summary()

    # 6. è¨“ç·´æ¨¡å‹
    print("\n--- æ­£åœ¨è¨“ç·´ã€Œæ·±åº¦ã€LSTM æ¨¡å‹... ---")
    model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        **LSTM_BASE_PARAMS
    )

    # 7. è©•ä¼°
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    
    print("æ¨¡å‹è¨“ç·´å®Œæˆã€‚")
    return model, accuracy, X_test, y_test

def plot_confusion_matrix(classifier, X_test, y_test, show_plot=False):
    """ (é€™æ˜¯æˆ‘å€‘å¸¶ã€Œé–‹é—œã€çš„ç¹ªåœ–å‡½æ•¸) """
    print("\n--- æ­¥é©Ÿ 6: æ­£åœ¨ç¹ªè£½æ··æ·†çŸ©é™£ ---")
    try:
        y_pred = (classifier.predict(X_test, verbose=0) > 0.5).astype(int)
        
        # (å°å‡ºæœ€çµ‚æº–ç¢ºç‡)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"\næ¨¡å‹åœ¨ã€Œæ¸¬è©¦é›†ã€ä¸Šçš„æº–ç¢ºç‡ (Accuracy): {accuracy:.2%}")
        print("\n--- è©³ç´°åˆ†é¡å ±å‘Š (Classification Report) ---")
        print(classification_report(y_test, y_pred, target_names=['è·Œ (0)', 'æ¼² (1)']))

        if show_plot:
            print("æ­£åœ¨é¡¯ç¤ºåœ–è¡¨...")
            fig, ax = plt.subplots(figsize=(10, 7))
            ax.set_title('æ··æ·†çŸ©é™£ (Confusion Matrix) - 1h LSTM')
            ConfusionMatrixDisplay.from_predictions(
                y_test, y_pred, ax=ax, cmap=plt.cm.Blues,
                display_labels=['å¯¦éš› è·Œ (0)', 'å¯¦éš› æ¼² (1)']
            )
            ax.xaxis.set_ticklabels(['é æ¸¬ è·Œ (0)', 'é æ¸¬ æ¼² (1)'])
            ax.yaxis.set_ticklabels(['å¯¦éš› è·Œ (0)', 'å¯¦éš› æ¼² (1)'])
            plt.show()
        else:
            print("ç¹ªåœ–é–‹é—œå·²é—œé–‰ (æœªå‚³å…¥ --plot)ã€‚")
    except Exception as e:
        print(f"ç¹ªè£½æ··æ·†çŸ©é™£æ™‚å‡ºéŒ¯: {e}")

def evaluate_existing_model_trend(symbol, version, raw_df):
    """ è©•ä¼°ã€Œç¾è¡Œã€æ¨¡å‹åœ¨ã€Œç›¸åŒã€æ•¸æ“šä¸Šçš„ Accuracyã€‚"""
    model_path = config.get_trend_model_path(symbol, version)
    config_path = model_path.replace('.keras', '_feature_config.json')
    
    if not os.path.exists(model_path) or not os.path.exists(config_path):
        print("--- æ‰¾ä¸åˆ°ç¾è¡Œæ¨¡å‹ï¼Œè·³éç«¶çˆ­æ¯”è¼ƒã€‚---")
        return 0.0 # è¿”å› 0.0 æº–ç¢ºç‡ï¼Œç¢ºä¿æ–°æ¨¡å‹ç²å‹

    try:
        print(f"--- è¼‰å…¥ç¾è¡Œæ¨¡å‹ ({version}) é€²è¡Œç«¶çˆ­æ¯”è¼ƒ... ---")
        
        # 1. è¼‰å…¥ç¾è¡Œæ¨¡å‹å’Œå®ƒçš„ç‰¹å¾µåƒæ•¸
        current_model = tf.keras.models.load_model(model_path)
        with open(config_path, 'r') as f:
            current_feature_config = json.load(f)
        
        print(f"ç¾è¡Œæ¨¡å‹çš„ç‰¹å¾µåƒæ•¸: {current_feature_config}")
        
        # 2. å‰µå»ºç‰¹å¾µ (ä½¿ç”¨ç¾è¡Œæ¨¡å‹è‡ªå·±çš„é…ç½®)
        df_features_old, features_list_old = create_features_trend(raw_df.copy(), **current_feature_config)
        
        # 3. æº–å‚™æ•¸æ“š (å¿…é ˆèˆ‡ build_and_train_lstm é‚è¼¯ 100% ç›¸åŒ)
        P = config.TREND_MODEL_PARAMS
        df_model_old = df_features_old.copy()
        
        # 4. (*** é—œéµï¼šä½¿ç”¨å„²å­˜çš„é…ç½®ä¾†å»ºç«‹ Label ***)
        # (æˆ‘å€‘å‡è¨­ Label ç¸½æ˜¯åŸºæ–¼ 'SMA'ï¼Œå¦‚æœä¸æ˜¯ï¼Œé€™è£¡éœ€è¦ä¿®æ”¹)
        df_model_old['target'] = (df_model_old['SMA'].shift(-P['FORECAST_HORIZON']) > df_model_old['SMA']).astype(int)
        df_model_old = df_model_old.dropna() 
        
        # 5. æ¨™æº–åŒ–
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_features_old = scaler.fit_transform(df_model_old[features_list_old])
        target_old = df_model_old['target'].values
        
        # 6. å»ºç«‹ 3D åºåˆ—
        X_seq_old, y_seq_old = create_sequences(scaled_features_old, target_old, lookback_window=P['LOOKBACK_WINDOW'])
        
        # 7. åˆ†å‰² (åªå–æ¸¬è©¦é›†)
        test_size = 0.2 
        split_index_old = int(len(X_seq_old) * (1 - test_size))
        X_test_old = X_seq_old[split_index_old:]
        y_test_old = y_seq_old[split_index_old:]
        
        # 8. è©•ä¼°ç¾è¡Œæ¨¡å‹
        loss, accuracy = current_model.evaluate(X_test_old, y_test_old, verbose=0)
        
        print(f"--- ç¾è¡Œæ¨¡å‹åœ¨ã€Œç•¶å‰æ•¸æ“šã€ä¸Šçš„ Accuracy: {accuracy:.4f} ---")
        return accuracy

    except Exception as e:
        print(f"ğŸ›‘ è¼‰å…¥æˆ–è©•ä¼°ç¾è¡Œæ¨¡å‹æ™‚å‡ºéŒ¯: {e}")
        return 0.0 # å¤±æ•—è¿”å› 0.0ï¼Œç¢ºä¿æ–°æ¨¡å‹ç²å‹
    
if __name__ == "__main__":
    
    # --- 3. å»ºç«‹ã€Œåƒæ•¸è§£æå™¨ã€ ---
    parser = argparse.ArgumentParser(description='è¨“ç·´ 1h LSTM è¶¨å‹¢æ¨¡å‹')
    
    parser.add_argument(
        '-s', '--symbol', 
        type=str, 
        required=True, # <-- *** å¿…é ˆæŒ‡å®š symbol ***
        help='è¦è¨“ç·´çš„äº¤æ˜“å° (ä¾‹å¦‚: ETH/USDT æˆ– BTC/USDT)'
    )
    parser.add_argument(
        '-l', '--limit', 
        type=int, 
        default=config.TREND_MODEL_TRAIN_LIMIT, 
        help=f'K ç·šç­†æ•¸ (é è¨­: {config.TREND_MODEL_TRAIN_LIMIT})'
    )
    parser.add_argument(
        '-v', '--version',
        type=str,
        default=config.TREND_MODEL_VERSION, # <-- å¾ config è®€å–é è¨­ç‰ˆæœ¬
        help=f'è¦è¨“ç·´çš„æ¨¡å‹ç‰ˆæœ¬ (é è¨­: {config.TREND_MODEL_VERSION})'
    )
    parser.add_argument(
        '-p', '--plot', 
        action='store_true', 
        help='(é–‹é—œ) è¨“ç·´å®Œæˆå¾Œï¼Œé¡¯ç¤ºæ··æ·†çŸ©é™£åœ–è¡¨ã€‚'
    )
    
    args = parser.parse_args()
    
    # --- 4. åŸ·è¡Œè¨“ç·´ ---
    print(f"--- é–‹å§‹åŸ·è¡Œ: {args.symbol} ({config.TREND_MODEL_TIMEFRAME}), è³‡æ–™é‡={args.limit} ---")
    
    # (ç¢ºä¿ models è³‡æ–™å¤¾å­˜åœ¨)
    os.makedirs(config.MODEL_DIR, exist_ok=True)
    
    # 1. ç²å–è³‡æ–™
    raw_df = fetch_data(symbol=args.symbol, timeframe=config.TREND_MODEL_TIMEFRAME, total_limit=args.limit)

    # è¨­å®šåƒæ•¸æ ¼å¼, ç”Ÿæˆåƒæ•¸çµ„åˆ
    f_type = {
        'lookback_window': 'discrete', 
        'forecast_horizon': 'discrete',
    }
    iterator = SearchIterator(TREND_SEARCH_SPACE, search_type='random', n_iter=30, format_types=f_type)

    print(f"--- ç¸½å…±éœ€è¦åŸ·è¡Œ {iterator.get_total_runs()} æ¬¡è¨“ç·´ ---")
    
    best_accuracy = 0.0
    best_model = None
    best_feature_params = None 
    best_X_test = None
    best_y_test = None

    FEATURE_KEYS = ['ema', 'sma', 'rsi', 'bbands']

    for i, params in enumerate(iterator):
        
        # 1a. åˆ†é›¢ç‰¹å¾µåƒæ•¸å’Œæ¨¡å‹åƒæ•¸
        feature_params = {k: params[k] for k in FEATURE_KEYS if k in params}
        # train_params = {k: params[k] for k in params.keys() if k not in FEATURE_KEYS}
    
        # 2. ç‰¹å¾µå·¥ç¨‹ (å¾ common_utils å¼•ç”¨)
        df_features, features_list = create_features_trend(raw_df, **feature_params)
        if df_features is None or features_list is None: 
            print(f"Iter {i+1:02d}/{iterator.get_total_runs()}: ç‰¹å¾µè¨ˆç®—å¤±æ•—ï¼Œè·³éã€‚")
            continue
        
        # 3. è¨“ç·´èˆ‡é æ¸¬
        best_classifier, accuracy, X_test, y_test = build_and_train_lstm(df_features, features_list)

        print(f"Iter {i+1:02d}/{iterator.get_total_runs()}: Accuracy={accuracy:.4f} ({feature_params}))")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model = best_classifier
            best_feature_params = feature_params
            best_X_test = X_test
            best_y_test = y_test

    if not best_model:
        print("ğŸ›‘ è¨“ç·´å¤±æ•—ï¼šæ¨¡å‹æœªè¢«å»ºç«‹ã€‚")
        exit()

    if best_accuracy < config.ABS_MIN_ACCURACY:
        print(f"\nâŒ è³ªé‡é–€ 1 å¤±æ•—ï¼æœ€ä½³ Accuracy ({best_accuracy:.4f}) æœªé”åˆ°çµ•å°æœ€ä½æ¨™æº– ({config.ABS_MIN_ACCURACY * 100}%)ã€‚ä¸å„²å­˜æ¨¡å‹ã€‚")
        exit()
    else:
        print(f"\nâœ… è³ªé‡é–€ 1 (çµ•å°æ¨™æº–) é€šéï¼")

    historical_accuracy = evaluate_existing_model_trend(args.symbol, args.version, raw_df)
    
    if best_accuracy <= historical_accuracy:
        print(f"\nâŒ è³ªé‡é–€ 2 å¤±æ•—ï¼æ–°æ¨¡å‹ Accuracy ({best_accuracy:.4f}) ä¸¦æœªå„ªäºç¾è¡Œæ¨¡å‹ ({historical_accuracy:.4f})ã€‚ä¸å„²å­˜æ¨¡å‹ã€‚")
        exit()
    else:
        print(f"\nâœ… è³ªé‡é–€ 2 (ç«¶çˆ­æ¨™æº–) é€šéï¼æ–°æ¨¡å‹ ({best_accuracy:.4f}) æˆåŠŸæ“Šæ•— ç¾è¡Œæ¨¡å‹ ({historical_accuracy:.4f})ã€‚")

    model_filename = config.get_trend_model_path(args.symbol, args.version)
    config_filename = config.get_trend_model_path(args.symbol, args.version).replace('.keras', '_feature_config.json')
    
    # 5. å„²å­˜æ¨¡å‹
    if best_model:
        print(f"\n--- æ­£åœ¨å„²å­˜ã€Œè¶¨å‹¢æ¨¡å‹ã€... ---")
        best_model.save(model_filename)
        print(f"æ¨¡å‹å„²å­˜å®Œç•¢ï¼({model_filename})")

    if best_feature_params:
        with open(config_filename, 'w') as f:
            json.dump(best_feature_params, f, indent=4)
        print(f"âœ… æœ€ä½³ç‰¹å¾µé…ç½®å„²å­˜å®Œç•¢ï¼š{config_filename}")

    # 4. ç¹ªè£½æ··æ·†çŸ©é™£ (æ ¹æ“š --plot åƒæ•¸)
    if best_model:
        plot_confusion_matrix(best_model, best_X_test, best_y_test, show_plot=args.plot)